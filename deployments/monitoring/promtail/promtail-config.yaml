apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: logging
  labels:
    app.kubernetes.io/name: promtail
    app.kubernetes.io/component: logging
data:
  promtail.yaml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0

    positions:
      filename: /tmp/positions.yaml

    clients:
      - url: http://loki:3100/loki/api/v1/push
        tenant_id: crm
        batchwait: 1s
        batchsize: 1048576
        timeout: 10s
        backoff_config:
          min_period: 500ms
          max_period: 5m
          max_retries: 10

    scrape_configs:
      # Kubernetes pod logs
      - job_name: kubernetes-pods
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          # Only scrape pods in crm namespace
          - source_labels: [__meta_kubernetes_namespace]
            action: keep
            regex: crm

          # Drop non-running pods
          - source_labels: [__meta_kubernetes_pod_phase]
            action: drop
            regex: (Pending|Succeeded|Failed|Unknown)

          # Use pod name as job label
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

          # Use namespace as namespace label
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace

          # Use container name as container label
          - source_labels: [__meta_kubernetes_pod_container_name]
            target_label: container

          # Use app label from pod
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            target_label: app

          # Use component label from pod
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_component]
            target_label: component

          # Set path to pod logs
          - source_labels: [__meta_kubernetes_pod_uid, __meta_kubernetes_pod_container_name]
            target_label: __path__
            separator: /
            replacement: /var/log/pods/*$1/*.log

        pipeline_stages:
          # Parse JSON logs
          - json:
              expressions:
                level: level
                msg: msg
                time: time
                service: service
                trace_id: trace_id
                span_id: span_id
                tenant_id: tenant_id
                user_id: user_id
                request_id: request_id
                method: method
                path: path
                status: status
                duration: duration
                error: error

          # Extract labels from parsed JSON
          - labels:
              level:
              service:
              tenant_id:
              method:
              status:

          # Set timestamp from log
          - timestamp:
              source: time
              format: RFC3339Nano
              fallback_formats:
                - RFC3339
                - "2006-01-02T15:04:05.999999999Z07:00"

          # Add output for message
          - output:
              source: msg

      # System logs (optional)
      - job_name: system
        static_configs:
          - targets:
              - localhost
            labels:
              job: system
              __path__: /var/log/syslog

      # CRM service specific logs
      - job_name: crm-services
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - crm
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_part_of]
            action: keep
            regex: crm-platform

          - source_labels: [__meta_kubernetes_pod_name]
            target_label: pod

          - source_labels: [__meta_kubernetes_pod_label_app_kubernetes_io_name]
            target_label: service

          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace

          - source_labels: [__meta_kubernetes_pod_uid, __meta_kubernetes_pod_container_name]
            target_label: __path__
            separator: /
            replacement: /var/log/pods/*$1/*.log

        pipeline_stages:
          - json:
              expressions:
                level: level
                msg: msg
                time: time
                service: service
                trace_id: trace_id
                tenant_id: tenant_id
                error: error

          - labels:
              level:
              service:
              tenant_id:

          - timestamp:
              source: time
              format: RFC3339Nano

          # Drop debug logs in production
          - match:
              selector: '{level="debug"}'
              stages:
                - drop:
                    expression: ".*"
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
  namespace: logging
  labels:
    app.kubernetes.io/name: promtail
    app.kubernetes.io/component: logging
spec:
  selector:
    matchLabels:
      app.kubernetes.io/name: promtail
  template:
    metadata:
      labels:
        app.kubernetes.io/name: promtail
        app.kubernetes.io/component: logging
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9080"
    spec:
      serviceAccountName: promtail
      securityContext:
        runAsUser: 0
        runAsGroup: 0
      containers:
        - name: promtail
          image: grafana/promtail:2.9.2
          imagePullPolicy: IfNotPresent
          args:
            - -config.file=/etc/promtail/promtail.yaml
          ports:
            - name: http-metrics
              containerPort: 9080
              protocol: TCP
          resources:
            requests:
              cpu: 50m
              memory: 64Mi
            limits:
              cpu: 200m
              memory: 256Mi
          env:
            - name: HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
          volumeMounts:
            - name: config
              mountPath: /etc/promtail
            - name: varlog
              mountPath: /var/log
              readOnly: true
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: pods
              mountPath: /var/log/pods
              readOnly: true
            - name: positions
              mountPath: /tmp
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
              add:
                - DAC_READ_SEARCH
      tolerations:
        - effect: NoSchedule
          operator: Exists
      volumes:
        - name: config
          configMap:
            name: promtail-config
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: pods
          hostPath:
            path: /var/log/pods
        - name: positions
          emptyDir: {}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: promtail
  namespace: logging
  labels:
    app.kubernetes.io/name: promtail
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: promtail
  labels:
    app.kubernetes.io/name: promtail
rules:
  - apiGroups: [""]
    resources:
      - nodes
      - nodes/proxy
      - services
      - endpoints
      - pods
    verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: promtail
  labels:
    app.kubernetes.io/name: promtail
subjects:
  - kind: ServiceAccount
    name: promtail
    namespace: logging
roleRef:
  kind: ClusterRole
  name: promtail
  apiGroup: rbac.authorization.k8s.io
